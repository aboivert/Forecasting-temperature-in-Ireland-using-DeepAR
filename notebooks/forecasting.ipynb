{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c423c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2b7a0ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, DeepAR, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "from pytorch_forecasting.metrics import SMAPE, MAE, NormalDistributionLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b34e02",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7f6ec265",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>station</th>\n",
       "      <th>series_idx</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-08 12:00:00</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>83.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-09 12:00:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>9.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-10 12:00:00</td>\n",
       "      <td>12.6</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-11 12:00:00</td>\n",
       "      <td>12.7</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>46.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-12 12:00:00</td>\n",
       "      <td>14.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>57.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-05-13 12:00:00</td>\n",
       "      <td>18.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-05-14 12:00:00</td>\n",
       "      <td>16.8</td>\n",
       "      <td>10.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-05-15 12:00:00</td>\n",
       "      <td>16.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-05-16 12:00:00</td>\n",
       "      <td>15.2</td>\n",
       "      <td>11.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>11.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-05-17 12:00:00</td>\n",
       "      <td>10.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.4</td>\n",
       "      <td>81.0</td>\n",
       "      <td>phoenix_park</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  temp  wetb  dewpt  vappr  rhum       station  \\\n",
       "0  2019-05-08 12:00:00   7.9   6.7    5.3    8.9  83.0  phoenix_park   \n",
       "1  2019-05-09 12:00:00   8.3   7.4    6.3    9.6  87.0  phoenix_park   \n",
       "2  2019-05-10 12:00:00  12.6   8.1    2.1    7.1  48.0  phoenix_park   \n",
       "3  2019-05-11 12:00:00  12.7   7.9    1.6    6.9  46.0  phoenix_park   \n",
       "4  2019-05-12 12:00:00  14.6  10.6    6.4    9.6  57.0  phoenix_park   \n",
       "5  2019-05-13 12:00:00  18.7  12.4    5.9    9.3  42.0  phoenix_park   \n",
       "6  2019-05-14 12:00:00  16.8  10.3    2.6    7.3  38.0  phoenix_park   \n",
       "7  2019-05-15 12:00:00  16.9  10.0    1.5    6.8  35.0  phoenix_park   \n",
       "8  2019-05-16 12:00:00  15.2  11.8    8.6   11.2  64.0  phoenix_park   \n",
       "9  2019-05-17 12:00:00  10.6   9.1    7.5   10.4  81.0  phoenix_park   \n",
       "\n",
       "   series_idx  time_idx  \n",
       "0           0         0  \n",
       "1           0         1  \n",
       "2           0         2  \n",
       "3           0         3  \n",
       "4           0         4  \n",
       "5           0         5  \n",
       "6           0         6  \n",
       "7           0         7  \n",
       "8           0         8  \n",
       "9           0         9  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We import the data of CAC 40 (obtained from Kaggle)\n",
    "data = pd.read_csv(\"data.csv\", index_col=[0])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "712aa7bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date          0\n",
       "temp          0\n",
       "wetb          0\n",
       "dewpt         0\n",
       "vappr         0\n",
       "rhum          0\n",
       "station       0\n",
       "series_idx    0\n",
       "time_idx      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d9ea9542",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.astype(dict(series_idx=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69696e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Essais modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "041a7860",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We consider 1000 time_idx.\n",
    "# 800 for training, 100 for validation, 100 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dd84f1c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset and dataloaders\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 20\n",
    "\n",
    "training_cutoff = 899\n",
    "\n",
    "context_length = max_encoder_length\n",
    "prediction_length = max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data = data[lambda x: x.time_idx <= 899],\n",
    "    time_idx=\"time_idx\", #indicateur de l'index de temps\n",
    "    target=\"temp\", #ce que l'on souhaite forecast\n",
    "    group_ids=[\"series_idx\"], #identificateur des différentes séries temporelles\n",
    "    static_categoricals=[\"series_idx\", \"station\"],  #variables catégorielles qui ne changent jamais\n",
    "    #categorical_encoders={\"idx\": NaNLabelEncoder().fit(data.idx)},\n",
    "    time_varying_unknown_reals=[\"temp\"], #variables qui changent au cours du temps\n",
    "    #time_varying_known_reals=[\"wetb\",\"dewpt\",\"vappr\",\"rhum\"], #covariables\n",
    "    max_encoder_length=context_length,\n",
    "    max_prediction_length=prediction_length,\n",
    "    #predict_mode=True\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training_cutoff + 1)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a92a946f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3198)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate baseline absolute error\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "SMAPE()(baseline_predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "93967aa8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "import pytorch_forecasting as ptf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b15a407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 432   \n",
      "3 | rnn                    | LSTM                   | 13.6 K\n",
      "4 | distribution_projector | Linear                 | 62    \n",
      "------------------------------------------------------------------\n",
      "14.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.1 K    Total params\n",
      "0.056     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurboivert/venv38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurboivert/venv38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  91%|▉| 153/169 [00:38<00:04,  3.98it/s, loss=2.16, v_num=9, train_loss\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  91%|▉| 154/169 [00:38<00:03,  3.97it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  92%|▉| 155/169 [00:39<00:03,  3.96it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  92%|▉| 156/169 [00:39<00:03,  3.95it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  93%|▉| 157/169 [00:39<00:03,  3.94it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  93%|▉| 158/169 [00:40<00:02,  3.93it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  94%|▉| 159/169 [00:40<00:02,  3.92it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  95%|▉| 160/169 [00:40<00:02,  3.92it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  95%|▉| 161/169 [00:41<00:02,  3.92it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  96%|▉| 162/169 [00:41<00:01,  3.91it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  96%|▉| 163/169 [00:41<00:01,  3.91it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  97%|▉| 164/169 [00:42<00:01,  3.90it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  98%|▉| 165/169 [00:42<00:01,  3.90it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  98%|▉| 166/169 [00:42<00:00,  3.90it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  99%|▉| 167/169 [00:42<00:00,  3.90it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0:  99%|▉| 168/169 [00:43<00:00,  3.90it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 0: 100%|█| 169/169 [00:43<00:00,  3.90it/s, loss=2.16, v_num=9, train_loss\u001B[A\n",
      "Epoch 1:  91%|▉| 153/169 [00:35<00:03,  4.36it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  91%|▉| 154/169 [00:35<00:03,  4.34it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  92%|▉| 155/169 [00:35<00:03,  4.33it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  92%|▉| 156/169 [00:36<00:03,  4.32it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  93%|▉| 157/169 [00:36<00:02,  4.31it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  93%|▉| 158/169 [00:36<00:02,  4.30it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  94%|▉| 159/169 [00:37<00:02,  4.29it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  95%|▉| 160/169 [00:37<00:02,  4.28it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  95%|▉| 161/169 [00:37<00:01,  4.28it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  96%|▉| 162/169 [00:37<00:01,  4.27it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  96%|▉| 163/169 [00:38<00:01,  4.26it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  97%|▉| 164/169 [00:38<00:01,  4.25it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  98%|▉| 165/169 [00:38<00:00,  4.23it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  98%|▉| 166/169 [00:39<00:00,  4.23it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  99%|▉| 167/169 [00:39<00:00,  4.21it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1:  99%|▉| 168/169 [00:39<00:00,  4.21it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 1: 100%|█| 169/169 [00:40<00:00,  4.21it/s, loss=2.1, v_num=9, train_loss_\u001B[A\n",
      "Epoch 2:  91%|▉| 153/169 [00:34<00:03,  4.41it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  91%|▉| 154/169 [00:35<00:03,  4.40it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  92%|▉| 155/169 [00:35<00:03,  4.38it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  92%|▉| 156/169 [00:35<00:02,  4.35it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  93%|▉| 157/169 [00:36<00:02,  4.34it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  93%|▉| 158/169 [00:36<00:02,  4.33it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  94%|▉| 159/169 [00:36<00:02,  4.32it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  95%|▉| 160/169 [00:37<00:02,  4.31it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  95%|▉| 161/169 [00:37<00:01,  4.30it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  96%|▉| 162/169 [00:37<00:01,  4.29it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  96%|▉| 163/169 [00:38<00:01,  4.28it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  97%|▉| 164/169 [00:38<00:01,  4.27it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  98%|▉| 165/169 [00:38<00:00,  4.24it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  98%|▉| 166/169 [00:39<00:00,  4.22it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  99%|▉| 167/169 [00:39<00:00,  4.21it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2:  99%|▉| 168/169 [00:40<00:00,  4.20it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 2: 100%|█| 169/169 [00:40<00:00,  4.19it/s, loss=2.01, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  91%|▉| 153/169 [00:34<00:03,  4.46it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  91%|▉| 154/169 [00:34<00:03,  4.45it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  92%|▉| 155/169 [00:34<00:03,  4.44it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  92%|▉| 156/169 [00:35<00:02,  4.44it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  93%|▉| 157/169 [00:35<00:02,  4.43it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  93%|▉| 158/169 [00:35<00:02,  4.43it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  94%|▉| 159/169 [00:35<00:02,  4.42it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  95%|▉| 160/169 [00:36<00:02,  4.42it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  95%|▉| 161/169 [00:36<00:01,  4.41it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  96%|▉| 162/169 [00:36<00:01,  4.40it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  96%|▉| 163/169 [00:37<00:01,  4.39it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  97%|▉| 164/169 [00:37<00:01,  4.38it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  98%|▉| 165/169 [00:37<00:00,  4.37it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  98%|▉| 166/169 [00:38<00:00,  4.36it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  99%|▉| 167/169 [00:38<00:00,  4.35it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3:  99%|▉| 168/169 [00:38<00:00,  4.35it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 3: 100%|█| 169/169 [00:38<00:00,  4.35it/s, loss=1.89, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  91%|▉| 153/169 [00:32<00:03,  4.67it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4:  91%|▉| 154/169 [00:33<00:03,  4.65it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  92%|▉| 155/169 [00:33<00:03,  4.64it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  92%|▉| 156/169 [00:33<00:02,  4.64it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  93%|▉| 157/169 [00:33<00:02,  4.62it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  93%|▉| 158/169 [00:34<00:02,  4.62it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  94%|▉| 159/169 [00:34<00:02,  4.61it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  95%|▉| 160/169 [00:34<00:01,  4.60it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  95%|▉| 161/169 [00:35<00:01,  4.59it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  96%|▉| 162/169 [00:35<00:01,  4.57it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  96%|▉| 163/169 [00:36<00:01,  4.51it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  97%|▉| 164/169 [00:36<00:01,  4.50it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  98%|▉| 165/169 [00:36<00:00,  4.49it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  98%|▉| 166/169 [00:36<00:00,  4.49it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  99%|▉| 167/169 [00:37<00:00,  4.48it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4:  99%|▉| 168/169 [00:37<00:00,  4.48it/s, loss=1.81, v_num=9, train_loss\u001B[A\n",
      "Epoch 4: 100%|█| 169/169 [00:37<00:00,  4.48it/s, loss=1.81, v_num=9, train_loss\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  91%|▉| 153/169 [00:34<00:03,  4.41it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  91%|▉| 154/169 [00:34<00:03,  4.40it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  92%|▉| 155/169 [00:35<00:03,  4.40it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  92%|▉| 156/169 [00:35<00:02,  4.39it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  93%|▉| 157/169 [00:35<00:02,  4.38it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  93%|▉| 158/169 [00:36<00:02,  4.38it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  94%|▉| 159/169 [00:36<00:02,  4.37it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  95%|▉| 160/169 [00:36<00:02,  4.37it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  95%|▉| 161/169 [00:36<00:01,  4.36it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  96%|▉| 162/169 [00:37<00:01,  4.35it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  96%|▉| 163/169 [00:37<00:01,  4.34it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  97%|▉| 164/169 [00:37<00:01,  4.34it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  98%|▉| 165/169 [00:38<00:00,  4.33it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  98%|▉| 166/169 [00:38<00:00,  4.33it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  99%|▉| 167/169 [00:38<00:00,  4.32it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5:  99%|▉| 168/169 [00:38<00:00,  4.32it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 5: 100%|█| 169/169 [00:39<00:00,  4.32it/s, loss=1.77, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  91%|▉| 153/169 [00:32<00:03,  4.73it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  91%|▉| 154/169 [00:32<00:03,  4.71it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  92%|▉| 155/169 [00:32<00:02,  4.70it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  92%|▉| 156/169 [00:33<00:02,  4.68it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  93%|▉| 157/169 [00:33<00:02,  4.67it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  93%|▉| 158/169 [00:33<00:02,  4.66it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  94%|▉| 159/169 [00:34<00:02,  4.64it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  95%|▉| 160/169 [00:34<00:01,  4.63it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  95%|▉| 161/169 [00:34<00:01,  4.62it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  96%|▉| 162/169 [00:35<00:01,  4.61it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  96%|▉| 163/169 [00:35<00:01,  4.60it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  97%|▉| 164/169 [00:35<00:01,  4.59it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  98%|▉| 165/169 [00:36<00:00,  4.58it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  98%|▉| 166/169 [00:36<00:00,  4.57it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  99%|▉| 167/169 [00:36<00:00,  4.56it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6:  99%|▉| 168/169 [00:36<00:00,  4.55it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 6: 100%|█| 169/169 [00:37<00:00,  4.55it/s, loss=1.73, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  91%|▉| 153/169 [00:36<00:03,  4.23it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  91%|▉| 154/169 [00:36<00:03,  4.20it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  92%|▉| 155/169 [00:37<00:03,  4.19it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  92%|▉| 156/169 [00:37<00:03,  4.17it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  93%|▉| 157/169 [00:37<00:02,  4.15it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  93%|▉| 158/169 [00:38<00:02,  4.13it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  94%|▉| 159/169 [00:38<00:02,  4.12it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  95%|▉| 160/169 [00:38<00:02,  4.10it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  95%|▉| 161/169 [00:39<00:01,  4.09it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  96%|▉| 162/169 [00:39<00:01,  4.08it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  96%|▉| 163/169 [00:40<00:01,  4.06it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  97%|▉| 164/169 [00:40<00:01,  4.06it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  98%|▉| 165/169 [00:41<00:01,  3.99it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  98%|▉| 166/169 [00:41<00:00,  3.97it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  99%|▉| 167/169 [00:42<00:00,  3.95it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7:  99%|▉| 168/169 [00:42<00:00,  3.95it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 7: 100%|█| 169/169 [00:42<00:00,  3.95it/s, loss=1.71, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  91%|▉| 153/169 [00:34<00:03,  4.38it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  91%|▉| 154/169 [00:35<00:03,  4.36it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  92%|▉| 155/169 [00:35<00:03,  4.36it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  92%|▉| 156/169 [00:35<00:02,  4.35it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  93%|▉| 157/169 [00:36<00:02,  4.34it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  93%|▉| 158/169 [00:36<00:02,  4.33it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  94%|▉| 159/169 [00:36<00:02,  4.32it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  95%|▉| 160/169 [00:37<00:02,  4.32it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  95%|▉| 161/169 [00:37<00:01,  4.31it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  96%|▉| 162/169 [00:37<00:01,  4.30it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  96%|▉| 163/169 [00:37<00:01,  4.30it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  97%|▉| 164/169 [00:38<00:01,  4.29it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  98%|▉| 165/169 [00:38<00:00,  4.28it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  98%|▉| 166/169 [00:38<00:00,  4.28it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  99%|▉| 167/169 [00:39<00:00,  4.27it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8:  99%|▉| 168/169 [00:39<00:00,  4.27it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 8: 100%|█| 169/169 [00:39<00:00,  4.27it/s, loss=1.68, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  91%|▉| 153/169 [00:38<00:04,  3.93it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  91%|▉| 154/169 [00:39<00:03,  3.91it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  92%|▉| 155/169 [00:39<00:03,  3.90it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  92%|▉| 156/169 [00:40<00:03,  3.89it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  93%|▉| 157/169 [00:40<00:03,  3.88it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  93%|▉| 158/169 [00:40<00:02,  3.87it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  94%|▉| 159/169 [00:41<00:02,  3.86it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  95%|▉| 160/169 [00:41<00:02,  3.85it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  95%|▉| 161/169 [00:41<00:02,  3.84it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  96%|▉| 162/169 [00:42<00:01,  3.83it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  96%|▉| 163/169 [00:42<00:01,  3.81it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  97%|▉| 164/169 [00:43<00:01,  3.81it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  98%|▉| 165/169 [00:43<00:01,  3.80it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  98%|▉| 166/169 [00:43<00:00,  3.79it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  99%|▉| 167/169 [00:44<00:00,  3.79it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9:  99%|▉| 168/169 [00:44<00:00,  3.78it/s, loss=1.69, v_num=9, train_loss\u001B[A\n",
      "Epoch 9: 100%|█| 169/169 [00:44<00:00,  3.78it/s, loss=1.69, v_num=9, train_loss\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  91%|▉| 153/169 [00:36<00:03,  4.20it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|                                        | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|                           | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  91%|▉| 154/169 [00:36<00:03,  4.19it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  92%|▉| 155/169 [00:37<00:03,  4.19it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  92%|▉| 156/169 [00:37<00:03,  4.18it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  93%|▉| 157/169 [00:37<00:02,  4.18it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  93%|▉| 158/169 [00:37<00:02,  4.18it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  94%|▉| 159/169 [00:38<00:02,  4.17it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  95%|▉| 160/169 [00:38<00:02,  4.17it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  95%|▉| 161/169 [00:38<00:01,  4.16it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  96%|▉| 162/169 [00:38<00:01,  4.16it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  96%|▉| 163/169 [00:39<00:01,  4.16it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  97%|▉| 164/169 [00:39<00:01,  4.15it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  98%|▉| 165/169 [00:39<00:00,  4.15it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  98%|▉| 166/169 [00:40<00:00,  4.15it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  99%|▉| 167/169 [00:40<00:00,  4.14it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10:  99%|▉| 168/169 [00:40<00:00,  4.14it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10: 100%|█| 169/169 [00:40<00:00,  4.14it/s, loss=1.68, v_num=9, train_los\u001B[A\n",
      "Epoch 10: 100%|█| 169/169 [00:40<00:00,  4.14it/s, loss=1.68, v_num=9, train_los\u001B[A\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback],\n",
    "    #limit_train_batches=50,\n",
    ")\n",
    "\n",
    "\n",
    "net = DeepAR.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.1,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    hidden_size=30,\n",
    "    rnn_layers=2,\n",
    "    loss=NormalDistributionLoss(),\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    net,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "68e4626e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = DeepAR.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e0e69342",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0964)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_model.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6c5e5dc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'decoder_time_idx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3800\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3799\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'decoder_time_idx'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [157], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mvalidation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_to_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseries_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1873\u001B[0m, in \u001B[0;36mTimeSeriesDataSet.x_to_index\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mx_to_index\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m   1867\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;124;03m    Decode dataframe index from x.\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \n\u001B[1;32m   1870\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[1;32m   1871\u001B[0m \u001B[38;5;124;03m        dataframe with time index column for first prediction and group ids\u001B[39;00m\n\u001B[1;32m   1872\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1873\u001B[0m     index_data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime_idx: \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdecoder_time_idx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[:, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()}\n\u001B[1;32m   1874\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_ids:\n\u001B[1;32m   1875\u001B[0m         index_data[\u001B[38;5;28mid\u001B[39m] \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroups\u001B[39m\u001B[38;5;124m\"\u001B[39m][:, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_ids\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28mid\u001B[39m)]\u001B[38;5;241m.\u001B[39mcpu()\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/core/frame.py:3805\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3804\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3805\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3807\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3804\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3805\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'decoder_time_idx'"
     ]
    }
   ],
   "source": [
    "validation.x_to_index(x)[\"series_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c6bbba53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_predictions, x = net.predict(val_dataloader, mode=\"raw\", return_x=False, n_samples=24, return_index=True, fast_dev_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "07667782",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'decoder_time_idx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3800\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3799\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'decoder_time_idx'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [156], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m series \u001B[38;5;241m=\u001B[39m \u001B[43mvalidation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_to_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseries_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m24\u001B[39m):  \u001B[38;5;66;03m# plot 10 examples\u001B[39;00m\n\u001B[1;32m      3\u001B[0m     best_model\u001B[38;5;241m.\u001B[39mplot_prediction(x, raw_predictions, idx\u001B[38;5;241m=\u001B[39midx, add_loss_to_title\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1873\u001B[0m, in \u001B[0;36mTimeSeriesDataSet.x_to_index\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mx_to_index\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m   1867\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;124;03m    Decode dataframe index from x.\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \n\u001B[1;32m   1870\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[1;32m   1871\u001B[0m \u001B[38;5;124;03m        dataframe with time index column for first prediction and group ids\u001B[39;00m\n\u001B[1;32m   1872\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1873\u001B[0m     index_data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime_idx: \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdecoder_time_idx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[:, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()}\n\u001B[1;32m   1874\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_ids:\n\u001B[1;32m   1875\u001B[0m         index_data[\u001B[38;5;28mid\u001B[39m] \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroups\u001B[39m\u001B[38;5;124m\"\u001B[39m][:, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_ids\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28mid\u001B[39m)]\u001B[38;5;241m.\u001B[39mcpu()\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/core/frame.py:3805\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3804\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3805\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3807\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/venv38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3804\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3805\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'decoder_time_idx'"
     ]
    }
   ],
   "source": [
    "series = validation.x_to_index(x)[\"series_idx\"]\n",
    "for idx in range(24):  # plot 10 examples\n",
    "    best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n",
    "    plt.suptitle(f\"Series: {series.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "373d62eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = TimeSeriesDataSet(\n",
    "    data_test,\n",
    "    time_idx=\"time_idx\", #indicateur de l'index de temps\n",
    "    target=\"temp\", #ce que l'on souhaite forecast\n",
    "    group_ids=[\"series_idx\"], #identificateur des différentes séries temporelles\n",
    "    static_categoricals=[\"series_idx\", \"station\"],  #variables catégorielles qui ne changent jamais\n",
    "    #categorical_encoders={\"idx\": NaNLabelEncoder().fit(data.idx)},\n",
    "    time_varying_unknown_reals=[\"temp\"], #variables qui changent au cours du temps\n",
    "    time_varying_known_reals=[\"wetb\",\"dewpt\",\"vappr\",\"rhum\"], #covariables\n",
    "    max_encoder_length=context_length,\n",
    "    max_prediction_length=prediction_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5297d96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_dataloader = test.to_dataloader(train=True, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf6c9370",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_cat': tensor([[[14,  2],\n",
       "          [14,  2],\n",
       "          [14,  2],\n",
       "          ...,\n",
       "          [14,  2],\n",
       "          [14,  2],\n",
       "          [14,  2]],\n",
       " \n",
       "         [[21, 22],\n",
       "          [21, 22],\n",
       "          [21, 22],\n",
       "          ...,\n",
       "          [21, 22],\n",
       "          [21, 22],\n",
       "          [21, 22]],\n",
       " \n",
       "         [[ 0, 19],\n",
       "          [ 0, 19],\n",
       "          [ 0, 19],\n",
       "          ...,\n",
       "          [ 0, 19],\n",
       "          [ 0, 19],\n",
       "          [ 0, 19]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[16, 10],\n",
       "          [16, 10],\n",
       "          [16, 10],\n",
       "          ...,\n",
       "          [16, 10],\n",
       "          [16, 10],\n",
       "          [16, 10]],\n",
       " \n",
       "         [[13,  1],\n",
       "          [13,  1],\n",
       "          [13,  1],\n",
       "          ...,\n",
       "          [13,  1],\n",
       "          [13,  1],\n",
       "          [13,  1]],\n",
       " \n",
       "         [[22, 16],\n",
       "          [22, 16],\n",
       "          [22, 16],\n",
       "          ...,\n",
       "          [22, 16],\n",
       "          [22, 16],\n",
       "          [22, 16]]]),\n",
       " 'encoder_cont': tensor([[[ 0.8132,  0.8204,  0.7739,  0.3904,  1.0772],\n",
       "          [ 0.2858,  0.1883,  0.0720, -0.3361,  0.6760],\n",
       "          [ 0.3517,  0.1883,  0.1133, -0.4399,  0.7854],\n",
       "          ...,\n",
       "          [-0.1757,  0.1309, -0.0106,  1.0132, -0.2359],\n",
       "          [ 0.1869,  0.2171,  0.1133,  0.0791,  0.4206],\n",
       "          [-0.0109,  0.1883,  0.0720,  0.5980,  0.0559]],\n",
       " \n",
       "         [[ 0.6154,  0.5044,  0.4023, -0.2323,  0.6309],\n",
       "          [ 1.2417,  1.3375,  1.3933,  0.9094,  1.1892],\n",
       "          [ 0.4836,  0.2458,  0.1546, -0.8551,  0.6867],\n",
       "          ...,\n",
       "          [ 0.9780,  1.0215,  0.9804,  0.4942,  0.9100],\n",
       "          [ 0.6154,  0.5044,  0.4023, -0.2323,  0.5750],\n",
       "          [ 0.0880,  0.0734, -0.0106, -0.1285, -0.3741]],\n",
       " \n",
       "         [[ 0.5495,  0.4469,  0.3610, -0.2323,  0.8046],\n",
       "          [ 1.3735,  1.0502,  1.0217, -0.8551,  1.8429],\n",
       "          [ 1.1428,  1.0790,  1.0630,  0.0791,  1.3562],\n",
       "          ...,\n",
       "          [-0.3075, -0.3001, -0.3822, -0.2323, -0.1038],\n",
       "          [-2.4501, -1.8515, -1.5797,  1.2208, -2.5697],\n",
       "          [-0.8349, -0.9034, -0.9190, -0.6475, -0.5256]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.5383, -0.7023, -0.7538, -0.8551,  0.1633],\n",
       "          [-0.2746,  0.1021, -0.0106,  1.3246, -0.1089],\n",
       "          [-0.4064, -0.2426, -0.3409,  0.3904, -0.0311],\n",
       "          ...,\n",
       "          [ 0.8132,  0.9640,  0.9391,  0.9094,  1.2910],\n",
       "          [ 0.4506,  0.3894,  0.2784, -0.1285,  1.1743],\n",
       "          [-0.2416,  0.0447, -0.0519,  1.0132,  0.0078]],\n",
       " \n",
       "         [[-0.2416, -0.4437, -0.5061, -0.8551, -1.3793],\n",
       "          [-0.6042, -0.8172, -0.8364, -0.9589, -1.6765],\n",
       "          [-0.1098, -1.0183, -1.0016, -2.8272, -0.7849],\n",
       "          ...,\n",
       "          [ 0.9780,  0.7629,  0.6914, -0.5437, -0.3796],\n",
       "          [ 1.2746,  1.1651,  1.1456, -0.0247, -0.2445],\n",
       "          [ 2.3624,  2.0270,  2.3430, -0.4399,  0.8092]],\n",
       " \n",
       "         [[-0.2746, -0.3288, -0.3822, -0.3361,  0.0414],\n",
       "          [ 0.0221, -0.0415, -0.1345, -0.3361,  0.4068],\n",
       "          [-0.3405, -0.5587, -0.6300, -0.9589,  0.1145],\n",
       "          ...,\n",
       "          [-0.8020, -0.4437, -0.5061,  1.0132, -0.9085],\n",
       "          [ 0.6154,  0.4469,  0.3610, -0.4399,  1.1010],\n",
       "          [ 1.2087,  1.3662,  1.4346,  1.2208,  1.3202]]]),\n",
       " 'encoder_target': tensor([[11.7000, 10.6000, 10.9000,  ...,  8.1000,  9.9000,  8.9000],\n",
       "         [11.6000, 12.6000, 11.7000,  ..., 12.1000, 11.5000,  9.8000],\n",
       "         [11.3000, 14.5000, 13.0000,  ...,  8.5000,  0.9000,  7.2000],\n",
       "         ...,\n",
       "         [ 8.3000,  7.6000,  7.8000,  ..., 11.2000, 10.9000,  7.9000],\n",
       "         [ 9.3000,  8.2000, 11.5000,  ..., 13.0000, 13.5000, 17.4000],\n",
       "         [ 8.7000,  9.7000,  8.9000,  ...,  6.1000, 11.6000, 12.2000]]),\n",
       " 'encoder_lengths': tensor([60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]),\n",
       " 'decoder_cat': tensor([[[14,  2],\n",
       "          [14,  2],\n",
       "          [14,  2],\n",
       "          ...,\n",
       "          [14,  2],\n",
       "          [14,  2],\n",
       "          [14,  2]],\n",
       " \n",
       "         [[21, 22],\n",
       "          [21, 22],\n",
       "          [21, 22],\n",
       "          ...,\n",
       "          [21, 22],\n",
       "          [21, 22],\n",
       "          [21, 22]],\n",
       " \n",
       "         [[ 0, 19],\n",
       "          [ 0, 19],\n",
       "          [ 0, 19],\n",
       "          ...,\n",
       "          [ 0, 19],\n",
       "          [ 0, 19],\n",
       "          [ 0, 19]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[16, 10],\n",
       "          [16, 10],\n",
       "          [16, 10],\n",
       "          ...,\n",
       "          [16, 10],\n",
       "          [16, 10],\n",
       "          [16, 10]],\n",
       " \n",
       "         [[13,  1],\n",
       "          [13,  1],\n",
       "          [13,  1],\n",
       "          ...,\n",
       "          [13,  1],\n",
       "          [13,  1],\n",
       "          [13,  1]],\n",
       " \n",
       "         [[22, 16],\n",
       "          [22, 16],\n",
       "          [22, 16],\n",
       "          ...,\n",
       "          [22, 16],\n",
       "          [22, 16],\n",
       "          [22, 16]]]),\n",
       " 'decoder_cont': tensor([[[-0.4394, -0.0702, -0.1758,  1.3246, -0.6006],\n",
       "          [ 0.5495,  0.4469,  0.3610, -0.1285,  0.9313],\n",
       "          [ 1.4065,  1.4524,  1.5172,  0.7018,  1.6607],\n",
       "          ...,\n",
       "          [-1.1316, -1.1620, -1.0842, -0.4399, -0.9653],\n",
       "          [-0.5712, -0.3863, -0.4235,  0.4942, -0.5641],\n",
       "          [-1.1316, -1.5355, -1.3319, -1.5817, -0.6735]],\n",
       " \n",
       "         [[-1.2964, -1.7366, -1.4971, -1.8931, -2.2164],\n",
       "          [-1.2634, -1.6217, -1.4145, -1.5817, -2.2164],\n",
       "          [-0.4064, -0.9034, -0.8777, -1.7893, -0.4857],\n",
       "          ...,\n",
       "          [-1.0986, -1.3344, -1.2081, -1.2703, -2.1048],\n",
       "          [-0.7360, -1.1332, -1.0842, -1.5817, -1.2115],\n",
       "          [-0.4064, -0.7023, -0.7126, -1.2703, -0.8207]],\n",
       " \n",
       "         [[-1.0657, -1.0471, -1.0016, -0.3361, -0.8501],\n",
       "          [ 0.9450,  0.9353,  0.8978,  0.2866,  1.0642],\n",
       "          [-0.7031, -0.9321, -0.9190, -1.1665, -0.2661],\n",
       "          ...,\n",
       "          [-0.6042, -1.1332, -1.0842, -1.9969,  0.0908],\n",
       "          [ 0.1210, -0.0415, -0.1345, -0.6475,  0.4477],\n",
       "          [ 0.5824,  0.5618,  0.4849, -0.0247,  0.8046]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4064, -0.1277, -0.2171,  0.8056, -0.1478],\n",
       "          [-2.3182, -1.9377, -1.6210,  0.3904, -2.3642],\n",
       "          [-1.9556, -2.0526, -1.7036, -1.0627, -1.5865],\n",
       "          ...,\n",
       "          [ 0.3187,  0.5618,  0.4849,  1.0132,  0.6688],\n",
       "          [-0.9338, -0.3863, -0.4648,  1.7398, -1.0032],\n",
       "          [-0.5712, -0.5874, -0.6300, -0.3361, -0.0311]],\n",
       " \n",
       "         [[ 2.6590,  2.6591,  3.2927,  1.2208,  0.6201],\n",
       "          [ 2.2635,  1.8834,  2.1366, -0.6475,  0.7822],\n",
       "          [ 2.4942,  1.8834,  2.1366, -1.4779,  1.2415],\n",
       "          ...,\n",
       "          [ 1.2746,  0.7629,  0.6914, -1.4779,  0.1337],\n",
       "          [ 1.8020,  1.4237,  1.4759, -0.8551,  0.4310],\n",
       "          [ 2.8898,  2.8027,  3.5405,  0.9094,  0.8903]],\n",
       " \n",
       "         [[ 0.9780,  0.8778,  0.8152, -0.1285,  1.4663],\n",
       "          [ 0.7143,  0.3894,  0.2784, -1.0627,  1.4663],\n",
       "          [-0.1098, -0.3001, -0.3822, -0.8551,  0.3702],\n",
       "          ...,\n",
       "          [-1.5271, -1.6217, -1.4145, -0.9589, -1.2374],\n",
       "          [-0.3075, -0.1564, -0.2583,  0.2866, -0.1778],\n",
       "          [-0.7690, -0.8747, -0.8777, -0.7513, -0.4336]]]),\n",
       " 'decoder_target': tensor([[ 7.1000, 11.3000, 13.3000,  ...,  6.1000,  7.2000,  6.9000],\n",
       "         [ 6.5000,  6.5000,  9.6000,  ...,  6.7000,  8.3000,  9.0000],\n",
       "         [ 6.2000, 12.1000,  8.0000,  ...,  9.1000, 10.2000, 11.3000],\n",
       "         ...,\n",
       "         [ 7.5000,  1.8000,  3.8000,  ...,  9.6000,  5.3000,  7.8000],\n",
       "         [16.7000, 17.3000, 19.0000,  ..., 14.9000, 16.0000, 17.7000],\n",
       "         [12.6000, 12.6000,  9.6000,  ...,  5.2000,  8.1000,  7.4000]]),\n",
       " 'decoder_lengths': tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]),\n",
       " 'decoder_time_idx': tensor([[965, 966, 967,  ..., 982, 983, 984],\n",
       "         [972, 973, 974,  ..., 989, 990, 991],\n",
       "         [977, 978, 979,  ..., 994, 995, 996],\n",
       "         ...,\n",
       "         [980, 981, 982,  ..., 997, 998, 999],\n",
       "         [964, 965, 966,  ..., 981, 982, 983],\n",
       "         [968, 969, 970,  ..., 985, 986, 987]]),\n",
       " 'groups': tensor([[14],\n",
       "         [21],\n",
       "         [ 0],\n",
       "         [12],\n",
       "         [ 7],\n",
       "         [15],\n",
       "         [16],\n",
       "         [18],\n",
       "         [16],\n",
       "         [23],\n",
       "         [12],\n",
       "         [ 3],\n",
       "         [ 5],\n",
       "         [22],\n",
       "         [19],\n",
       "         [17],\n",
       "         [ 1],\n",
       "         [18],\n",
       "         [ 9],\n",
       "         [ 6],\n",
       "         [23],\n",
       "         [ 8],\n",
       "         [ 4],\n",
       "         [21],\n",
       "         [ 2],\n",
       "         [ 2],\n",
       "         [10],\n",
       "         [ 3],\n",
       "         [14],\n",
       "         [18],\n",
       "         [ 7],\n",
       "         [ 0],\n",
       "         [21],\n",
       "         [ 8],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 3],\n",
       "         [22],\n",
       "         [ 6],\n",
       "         [15],\n",
       "         [ 5],\n",
       "         [19],\n",
       "         [ 7],\n",
       "         [23],\n",
       "         [22],\n",
       "         [ 5],\n",
       "         [23],\n",
       "         [13],\n",
       "         [ 3],\n",
       "         [ 2],\n",
       "         [21],\n",
       "         [ 3],\n",
       "         [ 6],\n",
       "         [ 3],\n",
       "         [18],\n",
       "         [12],\n",
       "         [ 9],\n",
       "         [16],\n",
       "         [15],\n",
       "         [21],\n",
       "         [22],\n",
       "         [17],\n",
       "         [ 9],\n",
       "         [ 9],\n",
       "         [21],\n",
       "         [ 3],\n",
       "         [10],\n",
       "         [ 2],\n",
       "         [22],\n",
       "         [ 9],\n",
       "         [12],\n",
       "         [ 1],\n",
       "         [20],\n",
       "         [23],\n",
       "         [ 3],\n",
       "         [ 2],\n",
       "         [12],\n",
       "         [ 2],\n",
       "         [ 6],\n",
       "         [18],\n",
       "         [23],\n",
       "         [ 3],\n",
       "         [22],\n",
       "         [18],\n",
       "         [ 9],\n",
       "         [ 8],\n",
       "         [18],\n",
       "         [14],\n",
       "         [ 5],\n",
       "         [ 8],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 5],\n",
       "         [23],\n",
       "         [18],\n",
       "         [15],\n",
       "         [14],\n",
       "         [ 5],\n",
       "         [12],\n",
       "         [ 5],\n",
       "         [ 9],\n",
       "         [17],\n",
       "         [ 7],\n",
       "         [17],\n",
       "         [ 1],\n",
       "         [19],\n",
       "         [22],\n",
       "         [13],\n",
       "         [14],\n",
       "         [ 6],\n",
       "         [ 3],\n",
       "         [13],\n",
       "         [10],\n",
       "         [ 4],\n",
       "         [ 1],\n",
       "         [15],\n",
       "         [19],\n",
       "         [ 8],\n",
       "         [ 8],\n",
       "         [13],\n",
       "         [23],\n",
       "         [23],\n",
       "         [10],\n",
       "         [ 0],\n",
       "         [ 2],\n",
       "         [ 9],\n",
       "         [ 4],\n",
       "         [19],\n",
       "         [23],\n",
       "         [21],\n",
       "         [20],\n",
       "         [ 6],\n",
       "         [18],\n",
       "         [16],\n",
       "         [23],\n",
       "         [ 8],\n",
       "         [ 4],\n",
       "         [17],\n",
       "         [23],\n",
       "         [ 9],\n",
       "         [22],\n",
       "         [13],\n",
       "         [12],\n",
       "         [ 1],\n",
       "         [11],\n",
       "         [13],\n",
       "         [ 4],\n",
       "         [12],\n",
       "         [11],\n",
       "         [22],\n",
       "         [19],\n",
       "         [14],\n",
       "         [22],\n",
       "         [ 8],\n",
       "         [ 8],\n",
       "         [ 8],\n",
       "         [ 5],\n",
       "         [ 7],\n",
       "         [10],\n",
       "         [ 2],\n",
       "         [12],\n",
       "         [17],\n",
       "         [10],\n",
       "         [15],\n",
       "         [17],\n",
       "         [17],\n",
       "         [15],\n",
       "         [14],\n",
       "         [ 7],\n",
       "         [18],\n",
       "         [ 9],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 2],\n",
       "         [14],\n",
       "         [12],\n",
       "         [ 8],\n",
       "         [15],\n",
       "         [21],\n",
       "         [ 1],\n",
       "         [14],\n",
       "         [14],\n",
       "         [10],\n",
       "         [ 6],\n",
       "         [15],\n",
       "         [15],\n",
       "         [12],\n",
       "         [22],\n",
       "         [16],\n",
       "         [14],\n",
       "         [23],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [12],\n",
       "         [ 0],\n",
       "         [ 1],\n",
       "         [16],\n",
       "         [13],\n",
       "         [ 0],\n",
       "         [20],\n",
       "         [ 6],\n",
       "         [ 3],\n",
       "         [ 5],\n",
       "         [15],\n",
       "         [ 2],\n",
       "         [16],\n",
       "         [20],\n",
       "         [ 1],\n",
       "         [ 9],\n",
       "         [11],\n",
       "         [21],\n",
       "         [ 7],\n",
       "         [ 4],\n",
       "         [ 2],\n",
       "         [12],\n",
       "         [20],\n",
       "         [ 7],\n",
       "         [ 7],\n",
       "         [ 4],\n",
       "         [16],\n",
       "         [14],\n",
       "         [ 4],\n",
       "         [ 8],\n",
       "         [14],\n",
       "         [16],\n",
       "         [18],\n",
       "         [19],\n",
       "         [15],\n",
       "         [11],\n",
       "         [ 5],\n",
       "         [ 9],\n",
       "         [12],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [22],\n",
       "         [ 5],\n",
       "         [ 4],\n",
       "         [ 2],\n",
       "         [ 4],\n",
       "         [20],\n",
       "         [ 2],\n",
       "         [11],\n",
       "         [13],\n",
       "         [12],\n",
       "         [12],\n",
       "         [19],\n",
       "         [17],\n",
       "         [ 0],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [ 1],\n",
       "         [ 8],\n",
       "         [ 5],\n",
       "         [18],\n",
       "         [ 9],\n",
       "         [20],\n",
       "         [ 0],\n",
       "         [10],\n",
       "         [17],\n",
       "         [ 4],\n",
       "         [23],\n",
       "         [17],\n",
       "         [ 0],\n",
       "         [ 4],\n",
       "         [ 6],\n",
       "         [18],\n",
       "         [16],\n",
       "         [13],\n",
       "         [ 9],\n",
       "         [12],\n",
       "         [17],\n",
       "         [13],\n",
       "         [16],\n",
       "         [ 4],\n",
       "         [21],\n",
       "         [21],\n",
       "         [15],\n",
       "         [20],\n",
       "         [11],\n",
       "         [16],\n",
       "         [15],\n",
       "         [22],\n",
       "         [20],\n",
       "         [20],\n",
       "         [17],\n",
       "         [12],\n",
       "         [19],\n",
       "         [13],\n",
       "         [14],\n",
       "         [ 6],\n",
       "         [20],\n",
       "         [17],\n",
       "         [ 4],\n",
       "         [ 8],\n",
       "         [23],\n",
       "         [10],\n",
       "         [10],\n",
       "         [ 5],\n",
       "         [15],\n",
       "         [ 3],\n",
       "         [14],\n",
       "         [13],\n",
       "         [ 8],\n",
       "         [ 8],\n",
       "         [11],\n",
       "         [11],\n",
       "         [23],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [21],\n",
       "         [ 1],\n",
       "         [ 6],\n",
       "         [15],\n",
       "         [ 3],\n",
       "         [ 2],\n",
       "         [16],\n",
       "         [11],\n",
       "         [17],\n",
       "         [19],\n",
       "         [20],\n",
       "         [ 6],\n",
       "         [22],\n",
       "         [ 1],\n",
       "         [17],\n",
       "         [10],\n",
       "         [ 0],\n",
       "         [ 5],\n",
       "         [ 3],\n",
       "         [19],\n",
       "         [11],\n",
       "         [11],\n",
       "         [13],\n",
       "         [ 5],\n",
       "         [14],\n",
       "         [20],\n",
       "         [ 3],\n",
       "         [21],\n",
       "         [ 2],\n",
       "         [ 7],\n",
       "         [ 9],\n",
       "         [ 8],\n",
       "         [ 4],\n",
       "         [ 2],\n",
       "         [ 4],\n",
       "         [ 7],\n",
       "         [21],\n",
       "         [ 4],\n",
       "         [22],\n",
       "         [16],\n",
       "         [11],\n",
       "         [11],\n",
       "         [20],\n",
       "         [ 0],\n",
       "         [ 1],\n",
       "         [ 1],\n",
       "         [ 8],\n",
       "         [23],\n",
       "         [ 9],\n",
       "         [16],\n",
       "         [20],\n",
       "         [18],\n",
       "         [18],\n",
       "         [11],\n",
       "         [23],\n",
       "         [11],\n",
       "         [ 4],\n",
       "         [18],\n",
       "         [18],\n",
       "         [ 5],\n",
       "         [10],\n",
       "         [17],\n",
       "         [ 2],\n",
       "         [10],\n",
       "         [15],\n",
       "         [18],\n",
       "         [18],\n",
       "         [11],\n",
       "         [14],\n",
       "         [20],\n",
       "         [11],\n",
       "         [15],\n",
       "         [21],\n",
       "         [ 8],\n",
       "         [13],\n",
       "         [22],\n",
       "         [19],\n",
       "         [16],\n",
       "         [10],\n",
       "         [19],\n",
       "         [14],\n",
       "         [21],\n",
       "         [20],\n",
       "         [10],\n",
       "         [21],\n",
       "         [21],\n",
       "         [19],\n",
       "         [19],\n",
       "         [ 6],\n",
       "         [13],\n",
       "         [ 9],\n",
       "         [11],\n",
       "         [ 2],\n",
       "         [23],\n",
       "         [14],\n",
       "         [ 1],\n",
       "         [ 3],\n",
       "         [19],\n",
       "         [ 6],\n",
       "         [ 0],\n",
       "         [ 1],\n",
       "         [ 6],\n",
       "         [ 4],\n",
       "         [ 0],\n",
       "         [ 1],\n",
       "         [17],\n",
       "         [12],\n",
       "         [10],\n",
       "         [ 7],\n",
       "         [11],\n",
       "         [ 9],\n",
       "         [ 9],\n",
       "         [22],\n",
       "         [ 1],\n",
       "         [19],\n",
       "         [10],\n",
       "         [ 0],\n",
       "         [10],\n",
       "         [13],\n",
       "         [ 1],\n",
       "         [ 5],\n",
       "         [ 0],\n",
       "         [19],\n",
       "         [ 0],\n",
       "         [19],\n",
       "         [15],\n",
       "         [ 1],\n",
       "         [ 5],\n",
       "         [20],\n",
       "         [ 3],\n",
       "         [17],\n",
       "         [10],\n",
       "         [22],\n",
       "         [16],\n",
       "         [ 6],\n",
       "         [ 1],\n",
       "         [16],\n",
       "         [ 2],\n",
       "         [22],\n",
       "         [ 0],\n",
       "         [ 3],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [ 9],\n",
       "         [ 3],\n",
       "         [13],\n",
       "         [ 7],\n",
       "         [12],\n",
       "         [ 2],\n",
       "         [16],\n",
       "         [21],\n",
       "         [ 7],\n",
       "         [ 7],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [10],\n",
       "         [19],\n",
       "         [23],\n",
       "         [ 2],\n",
       "         [ 1],\n",
       "         [ 3],\n",
       "         [17],\n",
       "         [ 0],\n",
       "         [ 7],\n",
       "         [11],\n",
       "         [16],\n",
       "         [13],\n",
       "         [22]]),\n",
       " 'target_scale': tensor([[ 8.7467,  2.7418],\n",
       "         [10.4700,  1.7912],\n",
       "         [ 8.8200,  3.0821],\n",
       "         [ 8.4083,  3.0488],\n",
       "         [ 9.5517,  2.6215],\n",
       "         [ 9.0517,  2.3698],\n",
       "         [ 7.8450,  2.5329],\n",
       "         [ 8.6733,  2.7159],\n",
       "         [ 8.5200,  2.7552],\n",
       "         [10.1267,  2.5287],\n",
       "         [ 8.9300,  3.0014],\n",
       "         [ 8.6600,  2.8858],\n",
       "         [ 8.4183,  3.0665],\n",
       "         [ 7.9967,  2.9267],\n",
       "         [ 9.8433,  2.8765],\n",
       "         [ 9.0900,  2.7285],\n",
       "         [ 9.2450,  2.2244],\n",
       "         [ 8.9000,  2.8639],\n",
       "         [ 9.5817,  2.9288],\n",
       "         [11.9050,  2.3443],\n",
       "         [ 9.3917,  2.4126],\n",
       "         [ 9.5033,  3.0835],\n",
       "         [ 8.9117,  3.0002],\n",
       "         [10.6633,  1.9723],\n",
       "         [ 9.5333,  2.3845],\n",
       "         [ 9.4033,  2.3812],\n",
       "         [ 8.8517,  2.5890],\n",
       "         [ 8.2867,  2.8751],\n",
       "         [ 9.1500,  3.1861],\n",
       "         [ 8.7917,  2.9773],\n",
       "         [ 9.3900,  2.4910],\n",
       "         [ 9.3717,  2.7885],\n",
       "         [10.2483,  1.8924],\n",
       "         [ 9.1133,  2.8484],\n",
       "         [ 8.3267,  3.0193],\n",
       "         [11.5733,  2.2741],\n",
       "         [ 9.3183,  2.4336],\n",
       "         [ 9.0517,  2.8267],\n",
       "         [ 8.7000,  2.8289],\n",
       "         [10.9883,  2.6325],\n",
       "         [ 8.4917,  2.3827],\n",
       "         [ 8.9700,  3.1185],\n",
       "         [ 9.1267,  2.7406],\n",
       "         [ 9.4200,  2.4946],\n",
       "         [ 9.9033,  2.2969],\n",
       "         [ 8.7117,  2.8312],\n",
       "         [ 9.4983,  3.3287],\n",
       "         [10.1883,  2.5995],\n",
       "         [15.4100,  3.1303],\n",
       "         [ 8.8283,  2.7329],\n",
       "         [ 9.5450,  2.4667],\n",
       "         [10.0883,  1.8430],\n",
       "         [ 8.9100,  2.8098],\n",
       "         [12.0517,  2.2828],\n",
       "         [ 8.9417,  2.8451],\n",
       "         [ 8.6517,  3.0585],\n",
       "         [ 9.2550,  3.1804],\n",
       "         [ 8.6517,  2.6460],\n",
       "         [ 7.8567,  2.5483],\n",
       "         [ 9.2050,  2.4514],\n",
       "         [10.7050,  2.0078],\n",
       "         [ 8.5783,  2.9136],\n",
       "         [ 9.7767,  2.7655],\n",
       "         [ 8.6450,  2.6638],\n",
       "         [ 9.1967,  2.7386],\n",
       "         [10.4750,  1.7897],\n",
       "         [ 9.2117,  2.9671],\n",
       "         [ 8.5483,  2.5908],\n",
       "         [ 9.6983,  2.5364],\n",
       "         [ 8.6517,  2.7856],\n",
       "         [ 9.1167,  2.6692],\n",
       "         [ 8.2050,  2.9329],\n",
       "         [10.1217,  2.4520],\n",
       "         [ 8.5433,  2.8163],\n",
       "         [ 9.3217,  2.3994],\n",
       "         [ 8.2767,  2.8626],\n",
       "         [ 8.8583,  2.2790],\n",
       "         [ 9.0483,  2.9349],\n",
       "         [ 9.3733,  2.3695],\n",
       "         [11.8633,  2.3103],\n",
       "         [ 8.7983,  2.8312],\n",
       "         [ 9.1933,  2.3179],\n",
       "         [ 8.1750,  2.8783],\n",
       "         [ 8.3900,  2.9506],\n",
       "         [ 8.7383,  2.7316],\n",
       "         [ 9.0833,  2.8101],\n",
       "         [ 9.1367,  2.8885],\n",
       "         [ 8.7233,  2.7770],\n",
       "         [ 9.1050,  3.1293],\n",
       "         [ 9.0317,  2.9127],\n",
       "         [ 9.2750,  2.9675],\n",
       "         [10.3050,  2.8977],\n",
       "         [ 9.3183,  2.4336],\n",
       "         [ 8.9650,  2.9179],\n",
       "         [ 9.5017,  2.4427],\n",
       "         [ 8.1767,  3.0210],\n",
       "         [ 9.1267,  2.4226],\n",
       "         [ 8.1217,  2.9461],\n",
       "         [ 9.0050,  2.9472],\n",
       "         [ 8.9583,  2.8967],\n",
       "         [ 9.3050,  3.1285],\n",
       "         [ 9.1567,  2.6642],\n",
       "         [ 9.9033,  2.8185],\n",
       "         [ 8.7767,  2.4372],\n",
       "         [10.1083,  2.9920],\n",
       "         [ 9.9633,  2.3020],\n",
       "         [ 9.2200,  2.7896],\n",
       "         [ 9.0267,  3.0451],\n",
       "         [15.1850,  3.2983],\n",
       "         [ 8.8250,  2.8125],\n",
       "         [11.3217,  2.4035],\n",
       "         [ 8.8467,  2.7034],\n",
       "         [15.1117,  3.3599],\n",
       "         [ 9.0900,  2.6763],\n",
       "         [ 8.7583,  2.8050],\n",
       "         [10.2100,  2.4887],\n",
       "         [ 9.4250,  2.7233],\n",
       "         [ 9.6950,  2.7396],\n",
       "         [ 8.6017,  3.1061],\n",
       "         [ 9.1000,  2.8353],\n",
       "         [14.3617,  3.7044],\n",
       "         [ 9.8133,  2.2523],\n",
       "         [10.0217,  2.4262],\n",
       "         [ 9.5567,  2.7290],\n",
       "         [ 9.2300,  2.7177],\n",
       "         [ 9.4533,  2.4019],\n",
       "         [ 9.0967,  2.6479],\n",
       "         [ 8.9900,  3.0767],\n",
       "         [ 9.9650,  3.0131],\n",
       "         [ 9.8217,  2.2611],\n",
       "         [10.1917,  1.8311],\n",
       "         [ 8.5000,  2.7310],\n",
       "         [10.8883,  2.7063],\n",
       "         [ 8.9200,  2.9352],\n",
       "         [ 8.1617,  2.6647],\n",
       "         [ 9.6400,  2.4644],\n",
       "         [ 9.0967,  2.8317],\n",
       "         [ 8.5000,  2.9730],\n",
       "         [ 9.6250,  2.9872],\n",
       "         [ 9.8300,  2.3802],\n",
       "         [ 9.0783,  2.6361],\n",
       "         [ 8.4283,  2.9656],\n",
       "         [14.6800,  3.5719],\n",
       "         [ 9.2183,  3.1561],\n",
       "         [ 9.8600,  2.3162],\n",
       "         [ 9.6917,  1.4941],\n",
       "         [15.0600,  3.3916],\n",
       "         [ 8.9433,  3.0186],\n",
       "         [ 8.9250,  2.8571],\n",
       "         [ 9.7833,  1.4863],\n",
       "         [ 8.0600,  2.9410],\n",
       "         [ 9.5733,  2.6518],\n",
       "         [ 8.6700,  2.7221],\n",
       "         [ 8.6150,  2.6905],\n",
       "         [ 8.6850,  3.1131],\n",
       "         [ 9.1433,  2.8684],\n",
       "         [ 9.1517,  2.8705],\n",
       "         [ 8.3533,  3.0564],\n",
       "         [ 9.7350,  2.8165],\n",
       "         [ 9.2117,  2.5839],\n",
       "         [ 9.7633,  2.5731],\n",
       "         [ 8.2850,  2.9894],\n",
       "         [ 9.8683,  2.8000],\n",
       "         [ 9.1067,  2.5071],\n",
       "         [ 8.5900,  2.4722],\n",
       "         [ 9.5400,  2.9852],\n",
       "         [10.1483,  3.0052],\n",
       "         [ 9.1033,  2.4022],\n",
       "         [ 9.0633,  3.1053],\n",
       "         [ 9.4667,  2.5295],\n",
       "         [ 9.0550,  3.0661],\n",
       "         [ 8.6017,  2.6392],\n",
       "         [ 8.6667,  2.7116],\n",
       "         [ 9.1067,  3.0276],\n",
       "         [ 9.4350,  2.4858],\n",
       "         [ 8.1967,  2.9972],\n",
       "         [ 9.1150,  3.0293],\n",
       "         [ 8.4717,  3.0082],\n",
       "         [ 9.3883,  2.6875],\n",
       "         [10.2833,  1.8996],\n",
       "         [ 9.8417,  2.3180],\n",
       "         [ 8.9217,  2.8655],\n",
       "         [ 8.9050,  2.8632],\n",
       "         [ 9.4850,  2.6747],\n",
       "         [10.2300,  2.8717],\n",
       "         [ 8.9850,  2.5492],\n",
       "         [ 9.1483,  2.4399],\n",
       "         [ 9.0250,  2.9352],\n",
       "         [ 9.0817,  3.1230],\n",
       "         [ 8.9367,  2.7926],\n",
       "         [ 8.9300,  2.9667],\n",
       "         [ 9.8683,  2.2773],\n",
       "         [ 9.3033,  2.7405],\n",
       "         [ 9.5017,  2.9482],\n",
       "         [ 9.0150,  2.9325],\n",
       "         [ 9.4617,  2.8496],\n",
       "         [ 9.7867,  2.4148],\n",
       "         [ 8.2100,  2.6770],\n",
       "         [15.3500,  3.1917],\n",
       "         [ 9.4517,  2.8474],\n",
       "         [ 8.6217,  2.8890],\n",
       "         [11.4400,  2.3565],\n",
       "         [ 8.8517,  2.7551],\n",
       "         [ 9.0467,  2.9904],\n",
       "         [ 9.1783,  2.4557],\n",
       "         [ 8.9400,  2.3288],\n",
       "         [ 8.7267,  2.7005],\n",
       "         [ 8.4150,  2.7261],\n",
       "         [10.0283,  2.3603],\n",
       "         [ 8.9833,  2.7932],\n",
       "         [ 9.8033,  1.5579],\n",
       "         [10.5417,  1.8309],\n",
       "         [ 9.2283,  2.5633],\n",
       "         [ 8.6483,  2.8947],\n",
       "         [ 9.1800,  2.4323],\n",
       "         [ 8.7233,  3.0882],\n",
       "         [ 7.9417,  2.8942],\n",
       "         [ 8.8817,  2.5163],\n",
       "         [ 9.3467,  2.4562],\n",
       "         [ 8.2283,  3.0445],\n",
       "         [ 8.1283,  2.6589],\n",
       "         [ 8.7217,  2.7821],\n",
       "         [ 8.0767,  3.0155],\n",
       "         [ 8.3717,  2.9635],\n",
       "         [ 8.7933,  2.9822],\n",
       "         [ 8.1133,  2.6497],\n",
       "         [ 8.8100,  2.7812],\n",
       "         [ 9.5917,  2.6711],\n",
       "         [ 9.0800,  2.4010],\n",
       "         [ 9.8117,  1.5609],\n",
       "         [ 8.9483,  2.9066],\n",
       "         [ 9.1950,  2.7451],\n",
       "         [ 8.9333,  2.8381],\n",
       "         [10.6950,  2.7870],\n",
       "         [ 9.3700,  2.5718],\n",
       "         [ 9.7117,  3.2125],\n",
       "         [ 8.1533,  2.9888],\n",
       "         [ 8.8150,  3.1707],\n",
       "         [ 8.5100,  2.6225],\n",
       "         [ 9.0750,  2.3780],\n",
       "         [ 8.5917,  2.6992],\n",
       "         [ 8.5433,  2.8163],\n",
       "         [ 9.4850,  2.4309],\n",
       "         [ 9.6950,  1.4995],\n",
       "         [15.0200,  3.4373],\n",
       "         [ 9.3133,  3.2572],\n",
       "         [ 9.0100,  2.9148],\n",
       "         [ 9.9300,  2.9881],\n",
       "         [ 9.8200,  2.7560],\n",
       "         [ 9.1917,  3.0476],\n",
       "         [ 9.4433,  3.0691],\n",
       "         [ 9.3933,  2.8134],\n",
       "         [10.1783,  2.4672],\n",
       "         [ 9.6050,  3.1464],\n",
       "         [ 9.4150,  3.2492],\n",
       "         [ 8.9133,  2.8681],\n",
       "         [ 9.1717,  2.7212],\n",
       "         [ 8.3883,  2.6676],\n",
       "         [ 8.9150,  3.1189],\n",
       "         [ 9.2150,  2.5844],\n",
       "         [ 9.9150,  2.8229],\n",
       "         [ 8.1417,  3.0120],\n",
       "         [ 9.8533,  2.2775],\n",
       "         [ 9.2367,  2.8465],\n",
       "         [ 9.6367,  3.0794],\n",
       "         [ 8.7417,  2.7998],\n",
       "         [11.7700,  2.2696],\n",
       "         [ 8.6817,  2.7239],\n",
       "         [ 9.0517,  2.7697],\n",
       "         [15.0083,  3.4465],\n",
       "         [ 8.9233,  2.7900],\n",
       "         [ 8.7617,  3.1021],\n",
       "         [ 9.7200,  2.7438],\n",
       "         [14.3117,  3.7172],\n",
       "         [ 7.8933,  2.6079],\n",
       "         [ 8.0550,  2.9870],\n",
       "         [10.4350,  1.7666],\n",
       "         [ 9.9967,  1.8022],\n",
       "         [ 9.1017,  2.5379],\n",
       "         [ 8.0350,  2.9239],\n",
       "         [ 9.7933,  1.5546],\n",
       "         [ 8.8350,  2.7276],\n",
       "         [ 8.7733,  2.5164],\n",
       "         [ 8.5400,  2.6979],\n",
       "         [ 8.3717,  2.6852],\n",
       "         [ 8.2283,  2.9083],\n",
       "         [ 9.7933,  2.8787],\n",
       "         [ 8.8800,  2.8241],\n",
       "         [ 9.1950,  2.7709],\n",
       "         [14.3100,  3.7013],\n",
       "         [ 8.3133,  3.0492],\n",
       "         [10.5400,  2.8672],\n",
       "         [ 8.8450,  3.0903],\n",
       "         [ 9.9050,  2.8229],\n",
       "         [ 8.7600,  2.8049],\n",
       "         [ 8.9233,  3.0436],\n",
       "         [ 9.9317,  2.2948],\n",
       "         [ 8.9500,  2.5909],\n",
       "         [ 8.5883,  2.5929],\n",
       "         [ 9.1083,  3.0274],\n",
       "         [ 9.3500,  2.6659],\n",
       "         [ 8.8400,  2.8826],\n",
       "         [ 8.6617,  3.0434],\n",
       "         [15.7250,  2.9197],\n",
       "         [ 8.4383,  2.9703],\n",
       "         [ 9.6883,  3.1933],\n",
       "         [ 9.7450,  1.4811],\n",
       "         [ 9.7983,  1.4859],\n",
       "         [ 9.8417,  2.2707],\n",
       "         [10.3967,  2.8685],\n",
       "         [ 9.0783,  2.5901],\n",
       "         [10.4700,  1.7872],\n",
       "         [ 9.8283,  2.3135],\n",
       "         [11.9483,  2.3419],\n",
       "         [ 9.2800,  2.5795],\n",
       "         [ 8.2933,  2.8610],\n",
       "         [ 8.8867,  2.2964],\n",
       "         [ 8.6450,  2.7352],\n",
       "         [ 9.7817,  1.5144],\n",
       "         [ 9.9967,  2.8522],\n",
       "         [ 9.7767,  2.7681],\n",
       "         [ 8.3217,  2.6538],\n",
       "         [10.7650,  2.7580],\n",
       "         [ 8.7367,  2.7842],\n",
       "         [ 9.2867,  2.2581],\n",
       "         [10.2067,  3.0749],\n",
       "         [ 9.1333,  2.5316],\n",
       "         [ 9.7400,  3.1946],\n",
       "         [ 8.5417,  3.0917],\n",
       "         [ 9.1767,  2.9428],\n",
       "         [ 9.7567,  2.7723],\n",
       "         [ 9.7267,  1.5396],\n",
       "         [ 9.7900,  1.4919],\n",
       "         [14.3017,  3.7026],\n",
       "         [ 9.4450,  3.2658],\n",
       "         [ 8.9283,  2.8643],\n",
       "         [ 7.8783,  2.9055],\n",
       "         [ 8.9633,  2.7381],\n",
       "         [10.6017,  1.9005],\n",
       "         [ 9.5150,  2.4547],\n",
       "         [ 9.6717,  2.7448],\n",
       "         [ 8.6233,  2.6404],\n",
       "         [ 9.0267,  2.9534],\n",
       "         [ 8.4633,  2.9653],\n",
       "         [ 9.5867,  2.4215],\n",
       "         [ 8.7800,  2.8676],\n",
       "         [ 9.6333,  2.7105],\n",
       "         [10.4483,  1.7781],\n",
       "         [ 8.5250,  2.6372],\n",
       "         [ 7.9017,  2.8565],\n",
       "         [ 9.1233,  2.8157],\n",
       "         [ 9.7617,  1.4883],\n",
       "         [ 9.8417,  1.4371],\n",
       "         [ 8.4200,  2.8902],\n",
       "         [ 9.2283,  2.7160],\n",
       "         [ 9.6650,  2.4136],\n",
       "         [ 9.3817,  2.3443],\n",
       "         [ 8.8733,  3.0302],\n",
       "         [ 9.9483,  2.3508],\n",
       "         [ 9.5317,  2.9111],\n",
       "         [ 7.8867,  2.5722],\n",
       "         [ 8.2567,  2.9153],\n",
       "         [ 8.2083,  3.0671],\n",
       "         [ 8.3883,  3.1175],\n",
       "         [ 9.7083,  1.5067],\n",
       "         [ 9.2883,  2.3596],\n",
       "         [ 9.7567,  1.5017],\n",
       "         [ 8.5850,  2.6457],\n",
       "         [ 9.1517,  3.1722],\n",
       "         [ 8.2983,  3.0804],\n",
       "         [ 9.1750,  2.9964],\n",
       "         [ 9.2000,  2.5851],\n",
       "         [ 9.7500,  2.7534],\n",
       "         [ 8.9117,  2.3310],\n",
       "         [ 9.1767,  2.5515],\n",
       "         [ 8.6550,  2.4809],\n",
       "         [ 8.9217,  2.8661],\n",
       "         [ 9.0883,  3.0841],\n",
       "         [ 9.7950,  1.5445],\n",
       "         [ 8.2317,  3.0471],\n",
       "         [ 8.4733,  2.7672],\n",
       "         [ 9.7783,  1.5101],\n",
       "         [ 9.1733,  2.4543],\n",
       "         [10.4950,  1.8000],\n",
       "         [ 9.1517,  2.8705],\n",
       "         [15.6517,  2.9549],\n",
       "         [ 8.9850,  3.0249],\n",
       "         [ 9.4567,  2.8724],\n",
       "         [ 8.3800,  2.7474],\n",
       "         [ 8.5167,  2.5417],\n",
       "         [ 9.5850,  2.8048],\n",
       "         [ 8.6283,  3.0383],\n",
       "         [10.0683,  1.8150],\n",
       "         [ 7.8750,  2.9015],\n",
       "         [ 9.3800,  2.5846],\n",
       "         [10.4400,  1.7704],\n",
       "         [10.3850,  1.8564],\n",
       "         [ 9.6650,  2.8558],\n",
       "         [ 9.7417,  2.7607],\n",
       "         [10.4750,  2.8800],\n",
       "         [14.5283,  3.6511],\n",
       "         [ 8.7633,  2.6734],\n",
       "         [ 9.7633,  1.5077],\n",
       "         [ 9.5133,  2.4625],\n",
       "         [10.0933,  2.5087],\n",
       "         [ 8.8017,  2.8468],\n",
       "         [ 9.3383,  2.3322],\n",
       "         [ 9.1983,  2.9552],\n",
       "         [ 9.3150,  2.7773],\n",
       "         [11.6583,  2.2678],\n",
       "         [ 8.6950,  3.0218],\n",
       "         [ 9.5983,  2.4060],\n",
       "         [11.2067,  2.5090],\n",
       "         [ 7.9600,  2.9334],\n",
       "         [ 8.7350,  3.0829],\n",
       "         [ 9.8917,  2.3285],\n",
       "         [ 9.8900,  2.7572],\n",
       "         [ 8.3183,  3.0266],\n",
       "         [ 9.1683,  2.5248],\n",
       "         [ 8.8417,  2.4733],\n",
       "         [ 9.8133,  1.4781],\n",
       "         [ 9.5833,  2.9307],\n",
       "         [ 9.2083,  2.7431],\n",
       "         [ 8.8567,  2.9003],\n",
       "         [ 9.8983,  2.3311],\n",
       "         [10.0133,  3.0642],\n",
       "         [ 8.6917,  2.5872],\n",
       "         [ 8.6117,  2.9695],\n",
       "         [ 9.2083,  2.5810],\n",
       "         [14.9217,  3.4838],\n",
       "         [ 9.8700,  2.3291],\n",
       "         [ 9.0900,  3.0231],\n",
       "         [ 9.4633,  2.8494],\n",
       "         [ 9.6600,  2.6612],\n",
       "         [ 9.3267,  2.9742],\n",
       "         [10.0500,  3.0893],\n",
       "         [ 8.5717,  2.4489],\n",
       "         [ 9.8883,  2.3282],\n",
       "         [ 8.7517,  3.1704],\n",
       "         [ 7.7950,  2.8630],\n",
       "         [ 8.7983,  2.7084],\n",
       "         [ 9.3933,  2.8715],\n",
       "         [ 9.5133,  2.6774],\n",
       "         [ 8.5267,  2.6845],\n",
       "         [ 8.0800,  2.6342],\n",
       "         [10.6167,  2.8156],\n",
       "         [ 9.8267,  2.3124],\n",
       "         [ 7.9683,  2.6202],\n",
       "         [ 9.4367,  2.4044],\n",
       "         [ 8.0133,  2.9481],\n",
       "         [ 9.1533,  3.0415],\n",
       "         [ 8.9467,  2.8451],\n",
       "         [14.8050,  3.5899],\n",
       "         [ 8.6817,  2.7320],\n",
       "         [ 9.1917,  2.4868],\n",
       "         [ 9.2900,  2.7084],\n",
       "         [ 8.3900,  2.8698],\n",
       "         [15.2767,  3.2361],\n",
       "         [ 9.2567,  2.5768],\n",
       "         [ 8.5200,  3.0963],\n",
       "         [ 9.3233,  2.4826],\n",
       "         [ 7.8567,  2.5469],\n",
       "         [10.4450,  1.7744],\n",
       "         [ 9.3617,  2.4692],\n",
       "         [ 9.4550,  2.4876],\n",
       "         [ 9.2700,  2.7670],\n",
       "         [ 9.3483,  2.8252],\n",
       "         [ 9.2950,  2.5130],\n",
       "         [ 9.7467,  2.7706],\n",
       "         [ 9.6833,  2.4758],\n",
       "         [ 9.7883,  2.5877],\n",
       "         [ 9.4883,  2.3611],\n",
       "         [ 8.6100,  2.8776],\n",
       "         [ 9.2933,  2.8553],\n",
       "         [ 9.6833,  3.1107],\n",
       "         [ 8.9867,  2.5423],\n",
       "         [ 9.7767,  1.4881],\n",
       "         [ 7.8800,  2.5717],\n",
       "         [14.4050,  3.7011],\n",
       "         [ 8.5867,  2.7370]])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36454c08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 20, 24])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e0339",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}